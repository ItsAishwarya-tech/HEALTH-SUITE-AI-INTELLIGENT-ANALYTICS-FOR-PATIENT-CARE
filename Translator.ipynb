{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e69b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file shape: (24758, 8)\n",
      "                                        cleaned_text\n",
      "0  include past medical history include past medi...\n",
      "1                             systemview system view\n",
      "2                              include rx include rx\n",
      "3                                             copies\n",
      "4                             systemview system view\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# File path\n",
    "data_folder = r\"C:\\Users\\asus\\Downloads\\health api\"\n",
    "file_path = os.path.join(data_folder, 'note.csv.gz')\n",
    "\n",
    "# Load the notes file\n",
    "nlp_df = pd.read_csv(file_path, compression='gzip')\n",
    "print(f\"Loaded file shape: {nlp_df.shape}\")\n",
    "\n",
    "# Combine text columns into a single field (adjust column names if needed)\n",
    "nlp_df['combined_text'] = nlp_df[['notetext', 'notevalue']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()                          # lowercase\n",
    "    text = re.sub(r'\\d+', '', text)             # remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)         # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "nlp_df['cleaned_text'] = nlp_df['combined_text'].apply(clean_text)\n",
    "\n",
    "# Check result\n",
    "print(nlp_df[['cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88307994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries for translation: 24758\n"
     ]
    }
   ],
   "source": [
    "# Create FAQ Pairs (Optional)\n",
    "# If your text already contains questions and answers, extract them.\n",
    "# If not, you can treat each cleaned note as a separate input to translate\n",
    "\n",
    "# For simplicity, use each note as one input\n",
    "faq_inputs = nlp_df['cleaned_text'].tolist()\n",
    "print(f\"Total entries for translation: {len(faq_inputs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eae95fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\programdata\\anaconda3\\lib\\site-packages (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a763fa6b",
   "metadata": {},
   "source": [
    "Tokenization / Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1d7df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028fe8bd426743b4a3d8b7dfb6ebe2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5602b8396f8448eb87062863c691e422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/812k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b09f79983674dd6bdd1169eef674c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec666912b5d46148282e22fe71c2e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbaa2060081f4c26b487b54e201cf60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc815ff6ea834b0dbe7d9e0ecc59ebfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80ee749d2a9467a95a061d582bdc837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961ca217e8404d19be0947b66462db3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-hi'  # English to Hindi\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(faq_inputs[:5], return_tensors=\"pt\", padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb1583",
   "metadata": {},
   "source": [
    "Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c52259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: include past medical history include past medical history\n",
      "HI: चिकित्सा क्षेत्र में पिछले इतिहास में चिकित्सा क्षेत्र शामिल है\n",
      "\n",
      "EN: systemview system view\n",
      "HI: तंत्र दृश्य\n",
      "\n",
      "EN: include rx include rx\n",
      "HI: आरएक्स मेक्स शामिल करें (x)\n",
      "\n",
      "EN: copies\n",
      "HI: प्रतियों की नक़ल करें\n",
      "\n",
      "EN: systemview system view\n",
      "HI: तंत्र दृश्य\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translated = model.generate(**inputs)\n",
    "translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "for en, hi in zip(faq_inputs[:5], translated_texts):\n",
    "    print(f\"EN: {en}\\nHI: {hi}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
